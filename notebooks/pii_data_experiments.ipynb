{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PII Detection Experiments with Hugging Face Dataset\n",
    "\n",
    "This notebook explores the [ai4privacy/pii-masking-200k](https://huggingface.co/datasets/ai4privacy/pii-masking-200k) dataset and tests OpenAI's ability to detect PII."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T19:24:50.261579Z",
     "start_time": "2026-01-12T19:24:50.258821Z"
    }
   },
   "source": [
    "# Install required packages if needed\n",
    "# !pip install datasets openai python-dotenv"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:36:36.568193Z",
     "start_time": "2026-01-13T16:36:32.189392Z"
    }
   },
   "source": "import os\nimport random\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom openai import OpenAI\nimport json\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file in parent directory (project root)\nenv_file = Path(__file__).parent.parent / '.env' if '__file__' in globals() else Path.cwd().parent / '.env'\n\nprint(f\"Looking for .env file at: {env_file}\")\nif env_file.exists():\n    print(f\"âœ“ Found .env file\")\n    load_dotenv(dotenv_path=env_file)\nelse:\n    print(f\"âš ï¸  .env file not found at {env_file}\")\n    print(\"   Will try loading from default locations\")\n    load_dotenv()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for .env file at: /Users/kodlan/workspaces/python/pii-llm/.env\n",
      "âœ“ Found .env file\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Loading the PII masking dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:36:41.601347Z",
     "start_time": "2026-01-13T16:36:38.791962Z"
    }
   },
   "source": "print(\"Loading dataset...\")\n\n# Get Hugging Face token from environment for faster downloads (optional)\nhf_token = os.getenv(\"HF_TOKEN\")\n\nif hf_token:\n    # Check if it's a placeholder value\n    if hf_token.startswith(\"your-\") or \"placeholder\" in hf_token.lower() or \"example\" in hf_token.lower():\n        print(\"âš ï¸  HF_TOKEN appears to be a placeholder value - will use unauthenticated access\")\n        print(\"   Edit your .env file and add your actual Hugging Face token for faster downloads\")\n        hf_token = None\n    else:\n        print(\"âœ“ Using HF_TOKEN for authenticated access (faster downloads)\")\nelse:\n    print(\"âš ï¸  No HF_TOKEN found - using unauthenticated access (slower)\")\n    print(\"   Get a token at https://huggingface.co/settings/tokens to speed up downloads\")\n\n# Load dataset with token if available\ndataset = load_dataset(\"ai4privacy/pii-masking-200k\", token=hf_token if hf_token else None)\ntrain_data = dataset[\"train\"]\n\nprint(f\"\\nDataset loaded successfully!\")\nprint(f\"Total number of samples: {len(train_data)}\")\nprint(f\"\\nColumn names: {train_data.column_names}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "âœ“ Using HF_TOKEN for authenticated access (faster downloads)\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Total number of samples: 209261\n",
      "\n",
      "Column names: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Sample Random Entries\n\nFiltering the dataset to English samples only, then selecting 5 random samples with a fixed seed for reproducibility."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:36:43.693285Z",
     "start_time": "2026-01-13T16:36:43.668391Z"
    }
   },
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Set number of samples\n",
    "number_samples = 2\n",
    "\n",
    "# Filter dataset to English only\n",
    "print(\"Filtering dataset to English samples only...\")\n",
    "english_data = train_data.filter(lambda x: x['language'] == 'en')\n",
    "print(f\"English samples: {len(english_data)} out of {len(train_data)} total\")\n",
    "\n",
    "# Sample random indices from English data\n",
    "sample_indices = random.sample(range(len(english_data)), number_samples)\n",
    "print(f\"\\nSelected indices from English dataset: {sample_indices}\")\n",
    "\n",
    "# Get the samples\n",
    "samples = [english_data[i] for i in sample_indices]\n",
    "print(f\"Successfully sampled {len(samples)} English entries\")\n",
    "\n",
    "# Verify all samples are English\n",
    "languages = [s['language'] for s in samples]\n",
    "print(f\"Languages: {languages}\")\n",
    "assert all(lang == 'en' for lang in languages), \"Not all samples are English!\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering dataset to English samples only...\n",
      "English samples: 43501 out of 209261 total\n",
      "\n",
      "Selected indices from English dataset: [41905, 7296]\n",
      "Successfully sampled 2 English entries\n",
      "Languages: ['en', 'en']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Samples\n",
    "\n",
    "Let's look at each sample to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:36:45.338568Z",
     "start_time": "2026-01-13T16:36:45.333214Z"
    }
   },
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SAMPLE {i} (ID: {sample['id']}, Language: {sample['language']})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“„ SOURCE TEXT (with PII):\")\n",
    "    print(f\"{sample['source_text']}\")\n",
    "    \n",
    "    print(f\"\\nðŸ”’ TARGET TEXT (masked):\")\n",
    "    print(f\"{sample['target_text']}\")\n",
    "    \n",
    "    print(f\"\\nðŸ·ï¸  GROUND TRUTH PII:\")\n",
    "    for pii_item in sample['privacy_mask']:\n",
    "        print(f\"  - {pii_item['label']:20s} : '{pii_item['value']}' (pos {pii_item['start']}-{pii_item['end']})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE 0 (ID: 207666, Language: en)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“„ SOURCE TEXT (with PII):\n",
      "Analyst, we need you to investigate further about the Mozilla/5.0 (Windows NT 5.1; WOW64; rv:12.2) Gecko/20100101 Firefox/12.2.4 case. Draw insights from this situation and propose how it can affect our company policies.\n",
      "\n",
      "ðŸ”’ TARGET TEXT (masked):\n",
      "[JOBTYPE], we need you to investigate further about the [USERAGENT] case. Draw insights from this situation and propose how it can affect our company policies.\n",
      "\n",
      "ðŸ·ï¸  GROUND TRUTH PII:\n",
      "  - JOBTYPE              : 'Analyst' (pos 0-7)\n",
      "  - USERAGENT            : 'Mozilla/5.0 (Windows NT 5.1; WOW64; rv:12.2) Gecko/20100101 Firefox/12.2.4' (pos 54-128)\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 1 (ID: 173057, Language: en)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“„ SOURCE TEXT (with PII):\n",
      "Our main blockchain consultant identified 13eBdNVhaUZ6RPaCbiD1jPWVnEKaRCUU and 0xffff97e7cbbdc819da9d6ae8ef80deaf4bf39cec as two significant cryptocurrency addresses to monitor considering their possible impact on our business continuity.\n",
      "\n",
      "ðŸ”’ TARGET TEXT (masked):\n",
      "Our main blockchain consultant identified [BITCOINADDRESS] and [ETHEREUMADDRESS] as two significant cryptocurrency addresses to monitor considering their possible impact on our business continuity.\n",
      "\n",
      "ðŸ·ï¸  GROUND TRUTH PII:\n",
      "  - BITCOINADDRESS       : '13eBdNVhaUZ6RPaCbiD1jPWVnEKaRCUU' (pos 42-74)\n",
      "  - ETHEREUMADDRESS      : '0xffff97e7cbbdc819da9d6ae8ef80deaf4bf39cec' (pos 79-121)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI API\n",
    "\n",
    "Configure the OpenAI client using the API key from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:36:47.103445Z",
     "start_time": "2026-01-13T16:36:47.058025Z"
    }
   },
   "source": [
    "# Get API key from environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate API key\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\\n\\n\")\n",
    "\n",
    "# Check if it's still the placeholder value from .env.example\n",
    "if api_key.startswith(\"your-\") or \"placeholder\" in api_key.lower() or \"example\" in api_key.lower():\n",
    "    raise ValueError(\"OPENAI_API_KEY appears to be the placeholder value from .env.example.\\n\\n\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "print(\"âœ“ OpenAI client initialized successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OpenAI client initialized successfully\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Send Samples to OpenAI for PII Detection\n",
    "\n",
    "Process each sample and get PII detection results from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "def detect_pii_with_model(sample, model_name, client, get_prompt: Callable[[str], str]):\n",
    "    \"\"\"\n",
    "    Detect PII in a sample using a specific OpenAI model.\n",
    "    \n",
    "    Args:\n",
    "        sample: Dataset sample containing 'source_text'\n",
    "        model_name: OpenAI model name (e.g., 'gpt-4o-mini', 'gpt-5.2')\n",
    "        client: OpenAI client instance\n",
    "        get_prompt: function that generates prompt\n",
    "    Returns:\n",
    "        dict: Result containing detected PII or error information\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = get_prompt(sample['source_text'])\n",
    "\n",
    "    result = {\n",
    "            \"sample_id\": sample['id'],\n",
    "            \"source_text\": sample['source_text'],\n",
    "            \"ground_truth\": sample['privacy_mask'],\n",
    "            \"model\": model_name\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a PII detection expert. Return only valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,  # deterministic output\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        llm_response = response.choices[0].message.content\n",
    "        \n",
    "        detected_pii = json.loads(llm_response)\n",
    "\n",
    "        result[\"detected_pii\"] = detected_pii\n",
    "        result[\"raw_response\"] = llm_response\n",
    "\n",
    "        return result\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        result[\"error\"] = \"Invalid JSON response\"\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = str(e)\n",
    "        return result\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:20:15.890316Z",
     "start_time": "2026-01-13T17:20:15.884638Z"
    }
   },
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Send Samples to OpenAI for PII Detection\n\nProcess each sample and get PII detection results from two OpenAI models:\n- **gpt-4o-mini**: Cost-effective baseline model\n- **gpt-5.2**: Latest and most capable model (as of Jan 2026)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:21:13.823608Z",
     "start_time": "2026-01-13T17:21:13.815456Z"
    }
   },
   "source": [
    "models_to_test = [\n",
    "    \"gpt-4o-mini\",  # Cost-effective baseline\n",
    "    \"gpt-5.2\"       # Latest flagship model\n",
    "]\n",
    "\n",
    "def print_result(result):\n",
    "    if 'error' in result:\n",
    "        print(f\"   âŒ Error: {result['error']}\")\n",
    "        print(f\"   âœ“ Raw Response: {result['raw_response'].strip().replace('\\n', '')}\")\n",
    "    else:\n",
    "        print(f\"   âœ“ Response: {\", \".join(result[\"detected_pii\"][\"pii\"])}\")\n",
    "\n",
    "def print_sample_data(sample):\n",
    "    # Show sample text\n",
    "    print(f\"\\nðŸ“„ Text: {sample['source_text']}\")\n",
    "\n",
    "    # Show expected PII (ground truth)\n",
    "    print(f\"\\nðŸ·ï¸  Expected PII ({len(sample['privacy_mask'])} items):\")\n",
    "    for pii_item in sample['privacy_mask']:\n",
    "        print(f\"   â€¢ {pii_item['label']:20s} : '{pii_item['value']}'\")\n",
    "\n",
    "def print_header(sample):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing Sample {i} (ID: {sample['id']})\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "def test_prompt_with_models(samples, get_prompt: Callable[[str], str]):\n",
    "    results_for_prompt_per_model: dict[str, list[object]] = {}\n",
    "    for i, sample in enumerate(samples):\n",
    "        # Test with each model\n",
    "        for model_name in models_to_test:\n",
    "            result = detect_pii_with_model(sample, model_name, client, get_prompt)\n",
    "            results_for_prompt_per_model.setdefault(model_name, []).append(result)\n",
    "    return results_for_prompt_per_model\n",
    "\n",
    "def print_results_for_prompt(results_for_prompt_per_model: dict[str, list[object]], samples, num_samples_to_print):\n",
    "    for i in range(num_samples_to_print):\n",
    "        print_header(samples[i])\n",
    "        print_sample_data(samples[i])\n",
    "\n",
    "        for model_name in models_to_test:\n",
    "            print(f\"\\nðŸ¤– Testing with {model_name}...\")\n",
    "            print_result(results_for_prompt_per_model[model_name][i])"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Simple PII Detection Prompt\n",
    "\n",
    "Create a prompt that asks the LLM to identify all PII in the text."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:25:16.957270Z",
     "start_time": "2026-01-13T17:25:16.953877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gety_pii_prompt_simplest(text):\n",
    "    \"\"\"Create a prompt for PII detection.\"\"\"\n",
    "    return f\"\"\"For the text provided below identify all personally identifiable information (PII) in it.\n",
    "               Return your response as a JSON array that contains all the portions of text identified as PII.\n",
    "               Only return the JSON, no additional text. JSON array should be named pii, for example {{'pii': ['value1', 'value']}}.\n",
    "               Text to analyze: {text}\n",
    "               \"\"\"\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Call simple prompt"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:25:59.266480Z",
     "start_time": "2026-01-13T17:25:55.413226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# process all the samples\n",
    "results_for_prompt_per_model = test_prompt_with_models(samples, gety_pii_prompt_simplest)"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:26:03.983988Z",
     "start_time": "2026-01-13T17:26:03.980282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print first N results\n",
    "print_results_for_prompt(results_for_prompt_per_model, samples, 2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing Sample 1 (ID: 207666)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“„ Text: Analyst, we need you to investigate further about the Mozilla/5.0 (Windows NT 5.1; WOW64; rv:12.2) Gecko/20100101 Firefox/12.2.4 case. Draw insights from this situation and propose how it can affect our company policies.\n",
      "\n",
      "ðŸ·ï¸  Expected PII (2 items):\n",
      "   â€¢ JOBTYPE              : 'Analyst'\n",
      "   â€¢ USERAGENT            : 'Mozilla/5.0 (Windows NT 5.1; WOW64; rv:12.2) Gecko/20100101 Firefox/12.2.4'\n",
      "\n",
      "ðŸ¤– Testing with gpt-4o-mini...\n",
      "   âœ“ Response: \n",
      "\n",
      "ðŸ¤– Testing with gpt-5.2...\n",
      "   âœ“ Response: \n",
      "\n",
      "================================================================================\n",
      "Processing Sample 1 (ID: 173057)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“„ Text: Our main blockchain consultant identified 13eBdNVhaUZ6RPaCbiD1jPWVnEKaRCUU and 0xffff97e7cbbdc819da9d6ae8ef80deaf4bf39cec as two significant cryptocurrency addresses to monitor considering their possible impact on our business continuity.\n",
      "\n",
      "ðŸ·ï¸  Expected PII (2 items):\n",
      "   â€¢ BITCOINADDRESS       : '13eBdNVhaUZ6RPaCbiD1jPWVnEKaRCUU'\n",
      "   â€¢ ETHEREUMADDRESS      : '0xffff97e7cbbdc819da9d6ae8ef80deaf4bf39cec'\n",
      "\n",
      "ðŸ¤– Testing with gpt-4o-mini...\n",
      "   âœ“ Response: \n",
      "\n",
      "ðŸ¤– Testing with gpt-5.2...\n",
      "   âœ“ Response: 13eBdNVhaUZ6RPaCbiD1jPWVnEKaRCUU, 0xffff97e7cbbdc819da9d6ae8ef80deaf4bf39cec\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate the stats"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:48:47.257852Z",
     "start_time": "2026-01-13T17:48:47.250072Z"
    }
   },
   "cell_type": "code",
   "source": "def calculate_stats_for_model(model_results):\n    \"\"\"\n    Calculate PII detection statistics for a single model.\n    \n    Args:\n        model_results: List of result objects from detect_pii_with_model\n        \n    Returns:\n        dict: Statistics including missed count, extra count, and details\n    \"\"\"\n    total_ground_truth = 0\n    total_detected = 0\n    total_missed = 0\n    total_extra = 0\n    sample_details = []\n    \n    for result in model_results:\n        if 'error' in result:\n            # Skip samples with errors\n            continue\n            \n        # Get ground truth values\n        ground_truth_values = set(item['value'] for item in result['ground_truth'])\n        \n        # Get detected values\n        detected_values = set(result.get('detected_pii', {}).get('pii', []))\n        \n        # Calculate missed (in ground truth but not detected)\n        missed = ground_truth_values - detected_values\n        \n        # Calculate extra (detected but not in ground truth)\n        extra = detected_values - ground_truth_values\n        \n        # Calculate correct (intersection)\n        correct = ground_truth_values & detected_values\n        \n        total_ground_truth += len(ground_truth_values)\n        total_detected += len(detected_values)\n        total_missed += len(missed)\n        total_extra += len(extra)\n        \n        sample_details.append({\n            'sample_id': result['sample_id'],\n            'ground_truth_count': len(ground_truth_values),\n            'detected_count': len(detected_values),\n            'correct_count': len(correct),\n            'missed_count': len(missed),\n            'extra_count': len(extra),\n            'missed_values': list(missed),\n            'extra_values': list(extra)\n        })\n    \n    # Calculate rates\n    recall = (total_ground_truth - total_missed) / total_ground_truth if total_ground_truth > 0 else 0\n    precision = (total_detected - total_extra) / total_detected if total_detected > 0 else 0\n    \n    return {\n        'total_ground_truth': total_ground_truth,\n        'total_detected': total_detected,\n        'total_missed': total_missed,\n        'total_extra': total_extra,\n        'recall': recall,\n        'precision': precision,\n        'sample_details': sample_details\n    }",
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": "def calculate_stats_for_all_models(results_per_model):\n    \"\"\"\n    Calculate PII detection statistics for all models.\n    \n    Args:\n        results_per_model: Dict mapping model name to list of result objects\n        \n    Returns:\n        dict: Statistics per model\n    \"\"\"\n    all_stats = {}\n    \n    for model_name, model_results in results_per_model.items():\n        stats = calculate_stats_for_model(model_results)\n        all_stats[model_name] = stats\n    \n    return all_stats\n\n\ndef print_stats(all_stats):\n    \"\"\"Print statistics in a formatted way.\"\"\"\n    for model_name, stats in all_stats.items():\n        print(f\"\\n{'='*60}\")\n        print(f\"Statistics for {model_name}\")\n        print(f\"{'='*60}\")\n        print(f\"Total ground truth PII: {stats['total_ground_truth']}\")\n        print(f\"Total detected PII:     {stats['total_detected']}\")\n        print(f\"Total missed:           {stats['total_missed']}\")\n        print(f\"Total extra (false +):  {stats['total_extra']}\")\n        print(f\"Recall:                 {stats['recall']:.2%}\")\n        print(f\"Precision:              {stats['precision']:.2%}\")\n        \n        print(f\"\\nPer-sample breakdown:\")\n        for detail in stats['sample_details']:\n            print(f\"  Sample {detail['sample_id']}:\")\n            print(f\"    Ground truth: {detail['ground_truth_count']}, Detected: {detail['detected_count']}\")\n            print(f\"    Correct: {detail['correct_count']}, Missed: {detail['missed_count']}, Extra: {detail['extra_count']}\")\n            if detail['missed_values']:\n                print(f\"    Missed values: {detail['missed_values']}\")\n            if detail['extra_values']:\n                print(f\"    Extra values: {detail['extra_values']}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:48:48.997946Z",
     "start_time": "2026-01-13T17:48:48.990879Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": "# Calculate and print statistics for all models\nall_stats = calculate_stats_for_all_models(results_for_prompt_per_model)\nprint_stats(all_stats)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:48:50.578393Z",
     "start_time": "2026-01-13T17:48:50.574159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Statistics for gpt-4o-mini\n",
      "============================================================\n",
      "Total ground truth PII: 4\n",
      "Total detected PII:     0\n",
      "Total missed:           4\n",
      "Total extra (false +):  0\n",
      "Recall:                 0.00%\n",
      "Precision:              0.00%\n",
      "\n",
      "Per-sample breakdown:\n",
      "  Sample 207666:\n",
      "    Ground truth: 2, Detected: 0\n",
      "    Correct: 0, Missed: 2, Extra: 0\n",
      "    Missed values: ['Analyst', 'Mozilla/5.0 (Windows NT 5.1; WOW64; rv:12.2) Gecko/20100101 Firefox/12.2.4']\n",
      "  Sample 173057:\n",
      "    Ground truth: 2, Detected: 0\n",
      "    Correct: 0, Missed: 2, Extra: 0\n",
      "    Missed values: ['0xffff97e7cbbdc819da9d6ae8ef80deaf4bf39cec', '13eBdNVhaUZ6RPaCbiD1jPWVnEKaRCUU']\n",
      "\n",
      "============================================================\n",
      "Statistics for gpt-5.2\n",
      "============================================================\n",
      "Total ground truth PII: 4\n",
      "Total detected PII:     2\n",
      "Total missed:           2\n",
      "Total extra (false +):  0\n",
      "Recall:                 50.00%\n",
      "Precision:              100.00%\n",
      "\n",
      "Per-sample breakdown:\n",
      "  Sample 207666:\n",
      "    Ground truth: 2, Detected: 0\n",
      "    Correct: 0, Missed: 2, Extra: 0\n",
      "    Missed values: ['Analyst', 'Mozilla/5.0 (Windows NT 5.1; WOW64; rv:12.2) Gecko/20100101 Firefox/12.2.4']\n",
      "  Sample 173057:\n",
      "    Ground truth: 2, Detected: 2\n",
      "    Correct: 2, Missed: 0, Extra: 0\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
