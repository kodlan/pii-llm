{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PII Detection Experiments with Hugging Face Dataset\n",
    "\n",
    "This notebook explores the [ai4privacy/pii-masking-200k](https://huggingface.co/datasets/ai4privacy/pii-masking-200k) dataset and tests OpenAI's ability to detect PII."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T19:24:50.261579Z",
     "start_time": "2026-01-12T19:24:50.258821Z"
    }
   },
   "source": [
    "# Install required packages if needed\n",
    "# !pip install datasets openai python-dotenv"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:36:36.568193Z",
     "start_time": "2026-01-13T16:36:32.189392Z"
    }
   },
   "source": "import os\nimport random\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom openai import OpenAI\nimport json\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file in parent directory (project root)\nenv_file = Path(__file__).parent.parent / '.env' if '__file__' in globals() else Path.cwd().parent / '.env'\n\nprint(f\"Looking for .env file at: {env_file}\")\nif env_file.exists():\n    print(f\"‚úì Found .env file\")\n    load_dotenv(dotenv_path=env_file)\nelse:\n    print(f\"‚ö†Ô∏è  .env file not found at {env_file}\")\n    print(\"   Will try loading from default locations\")\n    load_dotenv()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for .env file at: /Users/kodlan/workspaces/python/pii-llm/.env\n",
      "‚úì Found .env file\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "Loading the PII masking dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:36:41.601347Z",
     "start_time": "2026-01-13T16:36:38.791962Z"
    }
   },
   "source": "print(\"Loading dataset...\")\n\n# Get Hugging Face token from environment for faster downloads (optional)\nhf_token = os.getenv(\"HF_TOKEN\")\n\nif hf_token:\n    # Check if it's a placeholder value\n    if hf_token.startswith(\"your-\") or \"placeholder\" in hf_token.lower() or \"example\" in hf_token.lower():\n        print(\"‚ö†Ô∏è  HF_TOKEN appears to be a placeholder value - will use unauthenticated access\")\n        print(\"   Edit your .env file and add your actual Hugging Face token for faster downloads\")\n        hf_token = None\n    else:\n        print(\"‚úì Using HF_TOKEN for authenticated access (faster downloads)\")\nelse:\n    print(\"‚ö†Ô∏è  No HF_TOKEN found - using unauthenticated access (slower)\")\n    print(\"   Get a token at https://huggingface.co/settings/tokens to speed up downloads\")\n\n# Load dataset with token if available\ndataset = load_dataset(\"ai4privacy/pii-masking-200k\", token=hf_token if hf_token else None)\ntrain_data = dataset[\"train\"]\n\nprint(f\"\\nDataset loaded successfully!\")\nprint(f\"Total number of samples: {len(train_data)}\")\nprint(f\"\\nColumn names: {train_data.column_names}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "‚úì Using HF_TOKEN for authenticated access (faster downloads)\n",
      "\n",
      "Dataset loaded successfully!\n",
      "Total number of samples: 209261\n",
      "\n",
      "Column names: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Sample Random Entries\n\nFiltering the dataset to English samples only, then selecting 5 random samples with a fixed seed for reproducibility."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:36:43.693285Z",
     "start_time": "2026-01-13T16:36:43.668391Z"
    }
   },
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Set number of samples\n",
    "number_samples = 2\n",
    "\n",
    "# Filter dataset to English only\n",
    "print(\"Filtering dataset to English samples only...\")\n",
    "english_data = train_data.filter(lambda x: x['language'] == 'en')\n",
    "print(f\"English samples: {len(english_data)} out of {len(train_data)} total\")\n",
    "\n",
    "# Sample random indices from English data\n",
    "sample_indices = random.sample(range(len(english_data)), number_samples)\n",
    "print(f\"\\nSelected indices from English dataset: {sample_indices}\")\n",
    "\n",
    "# Get the samples\n",
    "samples = [english_data[i] for i in sample_indices]\n",
    "print(f\"Successfully sampled {len(samples)} English entries\")\n",
    "\n",
    "# Verify all samples are English\n",
    "languages = [s['language'] for s in samples]\n",
    "print(f\"Languages: {languages}\")\n",
    "assert all(lang == 'en' for lang in languages), \"Not all samples are English!\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering dataset to English samples only...\n",
      "English samples: 43501 out of 209261 total\n",
      "\n",
      "Selected indices from English dataset: [41905, 7296]\n",
      "Successfully sampled 2 English entries\n",
      "Languages: ['en', 'en']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Samples\n",
    "\n",
    "Let's look at each sample to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:36:45.338568Z",
     "start_time": "2026-01-13T16:36:45.333214Z"
    }
   },
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SAMPLE {i} (ID: {sample['id']}, Language: {sample['language']})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nüìÑ SOURCE TEXT (with PII):\")\n",
    "    print(f\"{sample['source_text']}\")\n",
    "    \n",
    "    print(f\"\\nüîí TARGET TEXT (masked):\")\n",
    "    print(f\"{sample['target_text']}\")\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è  GROUND TRUTH PII:\")\n",
    "    for pii_item in sample['privacy_mask']:\n",
    "        print(f\"  - {pii_item['label']:20s} : '{pii_item['value']}' (pos {pii_item['start']}-{pii_item['end']})\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE 0 (ID: 207666, Language: en)\n",
      "================================================================================\n",
      "\n",
      "üìÑ SOURCE TEXT (with PII):\n",
      "Analyst, we need you to investigate further about the Mozilla/5.0 (Windows NT 5.1; WOW64; rv:12.2) Gecko/20100101 Firefox/12.2.4 case. Draw insights from this situation and propose how it can affect our company policies.\n",
      "\n",
      "üîí TARGET TEXT (masked):\n",
      "[JOBTYPE], we need you to investigate further about the [USERAGENT] case. Draw insights from this situation and propose how it can affect our company policies.\n",
      "\n",
      "üè∑Ô∏è  GROUND TRUTH PII:\n",
      "  - JOBTYPE              : 'Analyst' (pos 0-7)\n",
      "  - USERAGENT            : 'Mozilla/5.0 (Windows NT 5.1; WOW64; rv:12.2) Gecko/20100101 Firefox/12.2.4' (pos 54-128)\n",
      "\n",
      "================================================================================\n",
      "SAMPLE 1 (ID: 173057, Language: en)\n",
      "================================================================================\n",
      "\n",
      "üìÑ SOURCE TEXT (with PII):\n",
      "Our main blockchain consultant identified 13eBdNVhaUZ6RPaCbiD1jPWVnEKaRCUU and 0xffff97e7cbbdc819da9d6ae8ef80deaf4bf39cec as two significant cryptocurrency addresses to monitor considering their possible impact on our business continuity.\n",
      "\n",
      "üîí TARGET TEXT (masked):\n",
      "Our main blockchain consultant identified [BITCOINADDRESS] and [ETHEREUMADDRESS] as two significant cryptocurrency addresses to monitor considering their possible impact on our business continuity.\n",
      "\n",
      "üè∑Ô∏è  GROUND TRUTH PII:\n",
      "  - BITCOINADDRESS       : '13eBdNVhaUZ6RPaCbiD1jPWVnEKaRCUU' (pos 42-74)\n",
      "  - ETHEREUMADDRESS      : '0xffff97e7cbbdc819da9d6ae8ef80deaf4bf39cec' (pos 79-121)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI API\n",
    "\n",
    "Configure the OpenAI client using the API key from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T16:36:47.103445Z",
     "start_time": "2026-01-13T16:36:47.058025Z"
    }
   },
   "source": [
    "# Get API key from environment variable\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate API key\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables.\\n\\n\")\n",
    "\n",
    "# Check if it's still the placeholder value from .env.example\n",
    "if api_key.startswith(\"your-\") or \"placeholder\" in api_key.lower() or \"example\" in api_key.lower():\n",
    "    raise ValueError(\"OPENAI_API_KEY appears to be the placeholder value from .env.example.\\n\\n\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "print(\"‚úì OpenAI client initialized successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì OpenAI client initialized successfully\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Send Samples to OpenAI for PII Detection\n",
    "\n",
    "Process each sample and get PII detection results from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "def detect_pii_with_model(sample, model_name, client, get_prompt: Callable[[str], str]):\n",
    "    \"\"\"\n",
    "    Detect PII in a sample using a specific OpenAI model.\n",
    "    \n",
    "    Args:\n",
    "        sample: Dataset sample containing 'source_text'\n",
    "        model_name: OpenAI model name (e.g., 'gpt-4o-mini', 'gpt-5.2')\n",
    "        client: OpenAI client instance\n",
    "        get_prompt: function that generates prompt\n",
    "    Returns:\n",
    "        dict: Result containing detected PII or error information\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = get_prompt(sample['source_text'])\n",
    "\n",
    "    result = {\n",
    "            \"sample_id\": sample['id'],\n",
    "            \"source_text\": sample['source_text'],\n",
    "            \"ground_truth\": sample['privacy_mask'],\n",
    "            \"model\": model_name\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a PII detection expert. Return only valid JSON.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,  # deterministic output\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        llm_response = response.choices[0].message.content\n",
    "        \n",
    "        detected_pii = json.loads(llm_response)\n",
    "\n",
    "        result[\"detected_pii\"] = detected_pii\n",
    "        result[\"raw_response\"] = llm_response\n",
    "\n",
    "        return result\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        result[\"error\"] = \"Invalid JSON response\"\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = str(e)\n",
    "        return result\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:20:15.890316Z",
     "start_time": "2026-01-13T17:20:15.884638Z"
    }
   },
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Send Samples to OpenAI for PII Detection\n\nProcess each sample and get PII detection results from two OpenAI models:\n- **gpt-4o-mini**: Cost-effective baseline model\n- **gpt-5.2**: Latest and most capable model (as of Jan 2026)"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:21:13.823608Z",
     "start_time": "2026-01-13T17:21:13.815456Z"
    }
   },
   "source": [
    "models_to_test = [\n",
    "    \"gpt-4o-mini\",  # Cost-effective baseline\n",
    "    \"gpt-5.2\"       # Latest flagship model\n",
    "]\n",
    "\n",
    "def print_result(result):\n",
    "    if 'error' in result:\n",
    "        print(f\"   ‚ùå Error: {result['error']}\")\n",
    "        print(f\"   ‚úì Raw Response: {result['raw_response'].strip().replace('\\n', '')}\")\n",
    "    else:\n",
    "        print(f\"   ‚úì Response: {\", \".join(result[\"detected_pii\"][\"pii\"])}\")\n",
    "\n",
    "def print_sample_data(sample):\n",
    "    # Show sample text\n",
    "    print(f\"\\nüìÑ Text: {sample['source_text']}\")\n",
    "\n",
    "    # Show expected PII (ground truth)\n",
    "    print(f\"\\nüè∑Ô∏è  Expected PII ({len(sample['privacy_mask'])} items):\")\n",
    "    for pii_item in sample['privacy_mask']:\n",
    "        print(f\"   ‚Ä¢ {pii_item['label']:20s} : '{pii_item['value']}'\")\n",
    "\n",
    "def print_header(sample):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing Sample {i} (ID: {sample['id']})\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "def test_prompt_with_models(samples, get_prompt: Callable[[str], str]):\n",
    "    results_for_prompt_per_model: dict[str, list[object]] = {}\n",
    "    for i, sample in enumerate(samples):\n",
    "        # Test with each model\n",
    "        for model_name in models_to_test:\n",
    "            result = detect_pii_with_model(sample, model_name, client, get_prompt)\n",
    "            results_for_prompt_per_model.setdefault(model_name, []).append(result)\n",
    "    return results_for_prompt_per_model\n",
    "\n",
    "def print_results_for_prompt(results_for_prompt_per_model: dict[str, list[object]], samples, num_samples_to_print):\n",
    "    for i in range(num_samples_to_print):\n",
    "        print_header(samples[i])\n",
    "        print_sample_data(samples[i])\n",
    "\n",
    "        for model_name in models_to_test:\n",
    "            print(f\"\\nü§ñ Testing with {model_name}...\")\n",
    "            print_result(results_for_prompt_per_model[model_name][i])"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Simple PII Detection Prompt\n",
    "\n",
    "Create a prompt that asks the LLM to identify all PII in the text."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:25:16.957270Z",
     "start_time": "2026-01-13T17:25:16.953877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gety_pii_prompt_simplest(text):\n",
    "    \"\"\"Create a prompt for PII detection.\"\"\"\n",
    "    return f\"\"\"For the text provided below identify all personally identifiable information (PII) in it.\n",
    "               Return your response as a JSON array that contains all the portions of text identified as PII.\n",
    "               Only return the JSON, no additional text. JSON array should be named pii, for example {{'pii': ['value1', 'value']}}.\n",
    "               Text to analyze: {text}\n",
    "               \"\"\"\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Call simple prompt"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:25:59.266480Z",
     "start_time": "2026-01-13T17:25:55.413226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# process all the samples\n",
    "results_for_prompt_per_model = test_prompt_with_models(samples, gety_pii_prompt_simplest)"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-13T17:26:03.983988Z",
     "start_time": "2026-01-13T17:26:03.980282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print first N results\n",
    "print_results_for_prompt(results_for_prompt_per_model, samples, 2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing Sample 1 (ID: 207666)\n",
      "================================================================================\n",
      "\n",
      "üìÑ Text: Analyst, we need you to investigate further about the Mozilla/5.0 (Windows NT 5.1; WOW64; rv:12.2) Gecko/20100101 Firefox/12.2.4 case. Draw insights from this situation and propose how it can affect our company policies.\n",
      "\n",
      "üè∑Ô∏è  Expected PII (2 items):\n",
      "   ‚Ä¢ JOBTYPE              : 'Analyst'\n",
      "   ‚Ä¢ USERAGENT            : 'Mozilla/5.0 (Windows NT 5.1; WOW64; rv:12.2) Gecko/20100101 Firefox/12.2.4'\n",
      "\n",
      "ü§ñ Testing with gpt-4o-mini...\n",
      "   ‚úì Response: \n",
      "\n",
      "ü§ñ Testing with gpt-5.2...\n",
      "   ‚úì Response: \n",
      "\n",
      "================================================================================\n",
      "Processing Sample 1 (ID: 173057)\n",
      "================================================================================\n",
      "\n",
      "üìÑ Text: Our main blockchain consultant identified 13eBdNVhaUZ6RPaCbiD1jPWVnEKaRCUU and 0xffff97e7cbbdc819da9d6ae8ef80deaf4bf39cec as two significant cryptocurrency addresses to monitor considering their possible impact on our business continuity.\n",
      "\n",
      "üè∑Ô∏è  Expected PII (2 items):\n",
      "   ‚Ä¢ BITCOINADDRESS       : '13eBdNVhaUZ6RPaCbiD1jPWVnEKaRCUU'\n",
      "   ‚Ä¢ ETHEREUMADDRESS      : '0xffff97e7cbbdc819da9d6ae8ef80deaf4bf39cec'\n",
      "\n",
      "ü§ñ Testing with gpt-4o-mini...\n",
      "   ‚úì Response: \n",
      "\n",
      "ü§ñ Testing with gpt-5.2...\n",
      "   ‚úì Response: 13eBdNVhaUZ6RPaCbiD1jPWVnEKaRCUU, 0xffff97e7cbbdc819da9d6ae8ef80deaf4bf39cec\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate the stats"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
